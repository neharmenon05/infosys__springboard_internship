import os
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# ============================================
# üîß Step 1: Configure LM Studio local endpoint
# ============================================
os.environ["OPENAI_API_BASE"] = "http://localhost:1234/v1"
os.environ["OPENAI_API_KEY"] = "lm-studio"  # placeholder key

# ============================================
# üìÑ Step 2: Load and process PDF
# ============================================
loader = PDFPlumberLoader("Basic_Home_Remedies.pdf")
docs = loader.load()
print(f"‚úÖ Loaded {len(docs)} pages.")

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
documents = splitter.split_documents(docs)
print(f"‚úÖ Created {len(documents)} chunks.")

# ============================================
# üîç Step 3: Build FAISS vector database
# ============================================
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(documents, embeddings)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# ============================================
# ü§ñ Step 4: Initialize LM Studio LLM
# ============================================
llm = ChatOpenAI(
    model="tinyllama-1.1b-chat-v1.0",
    temperature=0.7,
    openai_api_base=os.environ["OPENAI_API_BASE"],
    openai_api_key=os.environ["OPENAI_API_KEY"],
    request_timeout=60,
)

# ============================================
# üß† Step 5: Create prompt and chain
# ============================================
prompt = ChatPromptTemplate.from_template("""
You are a health expert assistant.
Use the provided context to answer the question clearly and accurately.
If the answer is not in the context, say "The information is not available in the provided context."

Context:
{context}

Question:
{question}

Answer:
""")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ============================================
# üí¨ Step 6: Hybrid Q&A (PDF + General Chat)
# ============================================
while True:
    query = input("\n‚ùì Enter your question (or type 'exit' to quit): ").strip()
    if query.lower() == "exit":
        print("üëã Exiting...")
        break

    # Step 1: Try answering from PDF context
    answer_from_pdf = rag_chain.invoke(query)

    # Step 2: If not found ‚Üí fallback to direct chat
    if "not available in the provided context" in answer_from_pdf.lower():
        print("\nüìö Info not in PDF. Asking LM Studio directly...")
        response = llm.invoke(query)
        print("üß† General Answer:", response.content)
    else:
        print("üí° Context-based Answer:", answer_from_pdf)
